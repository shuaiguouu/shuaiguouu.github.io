---
layout: post
title: "Kinect2.0 RGB-D human video-emotion dataset acquisition and processing "
subtitle: "Shuai Guo, Yang Wang, Bin Wang. Finished on 2018.07 "
date:   2018-7-18
comments: true
categories: Projects
---

* content
{:toc}

In this project, a RGB-D human video-emotion dataset collected by Kinect2.0 is introduced. Comparing with the single RGB-view used in most video-emotion datasets, Depth view has unique advantages by introducing spatial and depth information of the field. This datasets consists of 4k clips of RGB videos and 4k clips of Depth videos performed by professional actors, and belong to 7 emotion categories. Then we extracted the C3D feature for this dataset, and did some other processes. 

## My works
* Kinect2.0 research and algorithm design. 
* Dataset acquisition. 
* Data Processing. 

## 1. Algorithm design

### 1.1. Align with RGB view and Depth view
Video streams of RGB and Depth view provided by Kinect2.0 have different visions and resolutions. To meet the requirement of our project, I align the two views by resizing, intercepting and moving the two stream. After that, the skeleton points and their connections are drawn on the streams, as shown below. 

<div align="center"><img src="/images/duiqi.jpg"></div> 

### 1.2. Save data with high frame rate
Kinect2.0 has a frame rate of 30 Hz officially, while the highest frame rate I can detect is 20 Hz in practice. I optimize the algorithms, get all of the data firstly and save them in the end, instead of getting and saving at the same time. Then I get a stable frame rate of 15 Hz, which is close to the highest frame rate in practice. 

## 2. Dataset acquisition

We employed 24 professional actors to perform the 7 emotions. Three Kinect2.0 cameras are placed at front, left and right of the stage to record the performance of actors, as shown below. 

<div align="center"><img src="/images/stage.jpg"></div> 

Then three Kinect2.0 cameras are controlled by three computers. 
To know which frames recorded by the three computers are corresponding, I record the local time when each frame is saved, and align the frames with the difference of local times. 

## 3. Data processing

### 3.1. C3D feature extraction
Taking videos as sequences of frames, 3-Dimensional convolutional neural networks (3D-CNN) can capture the spatial and temporal dimensions along with discriminative information. <a href="http://vlg.cs.dartmouth.edu/c3d/">C3D</a> is a modified version of BVLC_caffe to support 3D-CNN. I make a fine-tune on C3D and extract features of our dataset. 

### 3.2. Get new streams
Besides, I get some new streams with the saved data. 

<div align="center"><img src="/images/new_stream.jpg"></div> 